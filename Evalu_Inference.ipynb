{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f93250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification, BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f31738",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_sent = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer_asp = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df3962b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_predictions_to_aspects(text, predictions, offset_mapping, input_ids, tokenizer):\n",
    "  aspects = []\n",
    "  current_aspect = None\n",
    "  \n",
    "  for i, (pred_label, (start_pos, end_pos)) in enumerate(zip(predictions, offset_mapping)):\n",
    "    # Skip special tokens and padding\n",
    "    if start_pos == 0 and end_pos == 0:\n",
    "      continue\n",
    "        \n",
    "    token = tokenizer.decode([input_ids[i]], skip_special_tokens=True)\n",
    "    \n",
    "    if pred_label == 1:  # B- (Beginning of aspect)\n",
    "      # Save previous aspect if exists\n",
    "      if current_aspect is not None:\n",
    "        aspects.append(current_aspect)\n",
    "      \n",
    "      # Start new aspect\n",
    "      current_aspect = {\n",
    "        \"term\": text[start_pos:end_pos],\n",
    "        \"from\": start_pos,\n",
    "        \"to\": end_pos,\n",
    "        \"tokens\": [token]\n",
    "      }\n",
    "        \n",
    "    elif pred_label == 2:  # I- (Inside aspect)\n",
    "      if current_aspect is not None:\n",
    "        # Extend current aspect\n",
    "        current_aspect[\"term\"] = text[current_aspect[\"from\"]:end_pos]\n",
    "        current_aspect[\"to\"] = end_pos\n",
    "        current_aspect[\"tokens\"].append(token)\n",
    "      \n",
    "      else:\n",
    "        current_aspect = {\n",
    "          \"term\": text[start_pos:end_pos],\n",
    "          \"from\": start_pos,\n",
    "          \"to\": end_pos,\n",
    "          \"tokens\": [token]\n",
    "        }\n",
    "        \n",
    "    elif pred_label == 0:  # O (Outside)\n",
    "      # End current aspect if exists\n",
    "      if current_aspect is not None:\n",
    "        aspects.append(current_aspect)\n",
    "        current_aspect = None\n",
    "  \n",
    "  if current_aspect is not None:\n",
    "    aspects.append(current_aspect)\n",
    "  \n",
    "  return aspects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d8f5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspects_extraction(text, model, tokenizer, max_length=64):\n",
    "  model.eval()\n",
    "  encoding = tokenizer(\n",
    "    text,\n",
    "    truncation = True,\n",
    "    padding = 'max_length',\n",
    "    max_length = max_length,\n",
    "    return_offsets_mapping = True,\n",
    "    return_tensors = 'pt'\n",
    "  )\n",
    "  input_ids = encoding[\"input_ids\"]\n",
    "  attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "  with torch.no_grad():\n",
    "    outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "    predicted_labels = torch.argmax(outputs.logits, dim=-1) # outputs.logits shape: [1, 64, 3]\n",
    "\n",
    "  aspects = convert_predictions_to_aspects(text,\n",
    "                                           predicted_labels[0].cpu().numpy(),\n",
    "                                           encoding[\"offset_mapping\"][0].cpu().numpy(),\n",
    "                                           input_ids[0].cpu().numpy(),\n",
    "                                           tokenizer)\n",
    "\n",
    "  return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b155aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(text, aspect, model, tokenizer):\n",
    "  inputs = tokenizer(\n",
    "      text, aspect,\n",
    "      return_tensors=\"pt\",\n",
    "      truncation=True,\n",
    "      padding=\"max_length\",\n",
    "      max_length=128\n",
    "  )\n",
    "  with torch.no_grad():\n",
    "      outputs = model(**inputs)\n",
    "      logits = outputs.logits\n",
    "      prediction = torch.argmax(logits, dim=1).item()\n",
    "  label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "  return label_map[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3104e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(text, aspects, sentiments):\n",
    "  print(f\"Text: {text}\")\n",
    "  print(f\"Found {len(aspects)} aspects:\")\n",
    "  \n",
    "  for i, aspect in enumerate(aspects, 1):\n",
    "    print(f\"  {i}. '{aspect['term']}' -> {sentiments[i-1]}\")\n",
    "      \n",
    "  print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d3d4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "AspectModel = BertForTokenClassification.from_pretrained(\"aspect_extraction_model\\checkpoint-12207\")\n",
    "SentimentModel = BertForSequenceClassification.from_pretrained(\"absa_model/checkpoint-10456\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1015fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Anyway, the owner was fake.\n",
      "Found 1 aspects:\n",
      "  1. 'owner' -> negative\n",
      "--------------------------------------------------\n",
      "Text: Owner is pleasant and entertaining.\n",
      "Found 1 aspects:\n",
      "  1. 'Owner' -> positive\n",
      "--------------------------------------------------\n",
      "Text: I have never in my life sent back food before, but I simply had to, and the waiter argued with me over this.\n",
      "Found 2 aspects:\n",
      "  1. 'food' -> negative\n",
      "  2. 'waiter' -> negative\n",
      "--------------------------------------------------\n",
      "Text: Although the restaurant itself is nice, I prefer not to go for the food.\n",
      "Found 1 aspects:\n",
      "  1. 'food' -> negative\n",
      "--------------------------------------------------\n",
      "Text: Creamy appetizers--taramasalata, eggplant salad, and Greek yogurt (with cuccumber, dill, and garlic) taste excellent when on warm pitas.\n",
      "Found 5 aspects:\n",
      "  1. 'Creamy appetizers' -> positive\n",
      "  2. '--taramasalata' -> positive\n",
      "  3. ', eggplant salad' -> positive\n",
      "  4. 'Greek yogurt (with cuccumber, dill, and garlic' -> positive\n",
      "  5. 'warm pitas' -> neutral\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\testing and evaluation\\Restaurants_Test_Data_PhaseA.csv\")\n",
    "test_data = test_data.tail(5)\n",
    "for _ , row in test_data.iterrows():\n",
    "  aspects = aspects_extraction(row['Sentence'], AspectModel, tokenizer_asp)\n",
    "  sentiments = []\n",
    "  for _, aspect in enumerate(aspects):\n",
    "    s = classify_sentiment(row['Sentence'], aspect['term'], SentimentModel, tokenizer_sent)\n",
    "    sentiments.append(s)\n",
    "  format_output(row['Sentence'], aspects, sentiments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-env)",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
