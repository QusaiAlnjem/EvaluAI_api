{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09b542f4-50b1-4e88-b1ba-f511e4ac409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7637c1",
   "metadata": {},
   "source": [
    "#### ABSA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "924e8b57-69c4-4117-aea2-9ddd9e284007",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Label encoding\n",
    "label_map = {\"positive\": 2, \"neutral\": 1, \"negative\": 0}\n",
    "\n",
    "def load_absa_dataset(folder_path):\n",
    "  all_data = []\n",
    "  for root, _, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "      if file.endswith(\".json\"):\n",
    "        path = os.path.join(root, file)\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "          content = json.load(f)\n",
    "          for entry in content:\n",
    "            text_tokens = entry[\"token\"]\n",
    "            full_text = \" \".join(text_tokens)\n",
    "\n",
    "            for aspect in entry.get(\"aspects\", []):\n",
    "              aspect_tokens = aspect[\"term\"]\n",
    "              aspect_text = \" \".join(aspect_tokens)\n",
    "              label = label_map.get(aspect[\"polarity\"], 1)  # default to neutral\n",
    "\n",
    "              # Format input as: sentence [SEP] aspect\n",
    "              encoding = tokenizer(\n",
    "                full_text,\n",
    "                aspect_text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=128,\n",
    "                return_tensors=\"pt\"\n",
    "              )\n",
    "              item = {\n",
    "                \"input_ids\": encoding[\"input_ids\"][0],\n",
    "                \"attention_mask\": encoding[\"attention_mask\"][0],\n",
    "                \"token_type_ids\": encoding[\"token_type_ids\"][0],\n",
    "                \"label\": label\n",
    "              }\n",
    "              all_data.append(item)\n",
    "  return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b15ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46472"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absa_data = load_absa_dataset(r\"C:\\Users\\HP\\Downloads\\ABSA\")\n",
    "len(absa_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd9d8e",
   "metadata": {},
   "source": [
    "#### PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f10323b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch dataset\n",
    "class ABSADataset(Dataset):\n",
    "  def __init__(self, data):\n",
    "    self.data = data  # list of dictionaries\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.data[idx]\n",
    "    return {\n",
    "      \"input_ids\": item[\"input_ids\"],\n",
    "      \"attention_mask\": item[\"attention_mask\"],\n",
    "      \"token_type_ids\": item[\"token_type_ids\"],\n",
    "      \"labels\": torch.tensor(item[\"label\"], dtype=torch.long)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9921fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(absa_data, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = ABSADataset(train_data)\n",
    "val_dataset = ABSADataset(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7bd46d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19031374",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./absa_model\",\n",
    "  num_train_epochs=2,\n",
    "  eval_strategy=\"epoch\",\n",
    "  logging_strategy=\"epoch\",\n",
    "  save_strategy=\"epoch\",\n",
    "  logging_dir=\"./logs\",\n",
    "  logging_steps=10,\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"accuracy\",\n",
    "  save_total_limit=1,\n",
    "  report_to=\"none\",  # disable WandB/MLFlow etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4bc307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "  logits, labels = eval_pred\n",
    "  predictions = np.argmax(logits, axis=1)\n",
    "  return {\n",
    "    \"accuracy\": accuracy_score(labels, predictions),\n",
    "    \"f1\": f1_score(labels, predictions, average=\"macro\")\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4d5150f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10456' max='10456' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10456/10456 14:34:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.492900</td>\n",
       "      <td>0.289729</td>\n",
       "      <td>0.923838</td>\n",
       "      <td>0.919667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.151800</td>\n",
       "      <td>0.166277</td>\n",
       "      <td>0.970310</td>\n",
       "      <td>0.968520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10456, training_loss=0.3223378970227172, metrics={'train_runtime': 52468.7071, 'train_samples_per_second': 1.594, 'train_steps_per_second': 0.199, 'total_flos': 5502227791527936.0, 'train_loss': 0.3223378970227172, 'epoch': 2.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=train_dataset,\n",
    "  eval_dataset=val_dataset,\n",
    "  compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b9da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(text, aspect):\n",
    "    inputs = tokenizer(\n",
    "        text, aspect,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "    label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "    return label_map[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0602a9b-faff-4813-876a-b2b5ab076bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\testing and evaluation\\Restaurants_Test_Data_PhaseA.csv\")\n",
    "test_data = test_data.tail()\n",
    "aspects = ['owner', 'Owner', 'food', 'waiter']\n",
    "for _ , row in test_data.iterrows():\n",
    "  aspects = classify_sentiment(row['Sentence'], )\n",
    "  format_output(row['Sentence'], aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce9efbb-a27b-4718-b4d2-10afe7d96c50",
   "metadata": {},
   "source": [
    "### To continue training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44118c6c-bc49-4f56-9872-358090e2ce18",
   "metadata": {},
   "source": [
    "##### 1- From checkpoint (same dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817587f1-b54d-43a6-a4e7-fe21b36bed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=\"Python Notebooks/absa_model/checkpoint-10456\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a142156-0706-4159-add2-510ba517a913",
   "metadata": {},
   "source": [
    "##### 2- Load Model (different dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67dd8aa6-7a7d-41e7-a1c9-399d60087b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"absa_model/checkpoint-10456\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bff7be-f069-41b5-b6f2-3658e50794dc",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90f1e9-acd2-4ffc-adff-6da8683f092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a different folder for deployment\n",
    "model.save_pretrained(\"./Sentemint_Model\")\n",
    "tokenizer.save_pretrained(\"./Sentemint_Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-env)",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
